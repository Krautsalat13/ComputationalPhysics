\section{Discussion}

We start the discussion by describing \refFig{fig: Probabilities}. To this end, we point out the main similarities and differences. (a) and (b) share the property that the probability function does not change its shape with time, i.e. the variance stays constant. In \refFig{fig: Averages} this becomes more obvious as the Variance in (a) and (b) stay constant at $1/2$ over time. However, it needs to be noted that the theoretical values do not exactly match with our simulation, this is not only restricted to (a) and (b) but to all the results. This, we investigate later. The reason why (a) and (b) are so different from the other cases is due to them being coherent states. While (a) depicts the scenario of the vacuum state itself, (b) describes a coherent state with non-zero $\alpha$.\footnote{$\alpha$ depicts the amplitude and phase of a coherent state} The interesting property of coherent states is that they resemble classical behavior. Imagine a ball in the classical harmonic oscillator. If the initial position of the ball is at the minimum, we do not expect anything to happen with the ball. This is analogous to our case (a). If, however, the ball is slightly displaced from the minimum, the ball starts to oscillate to the left and right, similar to our case (b). Additionally, the fact that the wave function is preserving its shape, is because coherent states stay coherent in the quantum harmonic oscillator. This in particular is very interesting, as these states are not eigenstates of our Hamiltonian. In (c), the behavior is striking, as it is the only case, where the average position stays constant, that is at 0, but the variance oscillates over time. The frequency at which the variance oscillates is $2\Omega$. Also, the difference between (a) and (c) is only the $\sigma$, but contrary to (a) the variance oscillates. This is due to \refEq{eq: xx sol}, where for $\Omega,\sigma = 1$, the variance is equal to a constant. But there is an underlying relation between $\Omega$ and $\sigma$, that governs the cases when the variance does not change over time. This relation can be derived from \refEq{eq: xx sol} when the constants in front of the $\sin^2(\Omega t)$ and $\cos^2(\Omega t)$ are the same, or simply put
\begin{equation}
	{1\over \sigma^2 \Omega^2} = \sigma^2 \quad \rightarrow \quad \Omega \sigma^2 = 1.
\end{equation}
We can see that our initial conditions for (a), satisfy this relation.\\

Considering (d) and (e), we see they are very similar. In both cases, we have a wave packet that is initially displaced from 0, which is why it oscillates back and forth. While (d) starts at $x_0 = 1$, (e) starts at $x_0 = 2$. Comparing the average positions in \refFig{fig: Averages} for both (d) and (e), one sees that in both cases the frequency is the same. This was also shown in the theoretical derivation (see \refEq{eq: x sol}), that the frequency of the average does only depend on $\Omega$. This behavior matches our expectations from classical mechanics. Independent of the location of the ball, it oscillates with the same frequency. Other than the mean position being the actual position of the ball, there is no difference in the solution for the position. Turning to the variances of (d) and (e), one can see that similar to (c), how it oscillates too. However, for (d) and (e) $\Omega$ was chosen twice as large as in (c), which is why the variance oscillates with double the frequency of (c). The difference between (d) and (e) is, that for choosing a larger $\sigma$, the amplitude of the variance oscillation is larger.\\

After having discussed the simulation results and comparing them with each other, we turn to the theoretical expectation. In \refFig{fig: Averages}, additionally to our simulation the theoretical results derived in section \ref{sec: theo} are plotted as the dashed line. In (a), we can see how the theoretical expectation for $\braket{x(t)}$ matches the results very well. However, for the variance, it seems like our simulation oscillates above the expectation. These are probably artifacts due to discretizations and approximations, as we have seen similar behavior in the exercise about "Molecular Dynamics Simulation". For further investigation, \refFig{fig: difference} shows the differences between the simulation and theoretical result. Here, our hypothesis of oscillation above the theoretical expectation is proved to be true. Interestingly, the oscillation frequency coincides with the frequency $\Omega$. In (b), similar to (a), the variance oscillates although again the theoretical expectation should be constant. But in both (a) and (b), it seems that the amplitude of the oscillation does not change over time. For (c), (d), and (e), the difference between simulation and theory, oscillates, but the amplitude increases gradually. For the mean position, we have similar behavior. In (b), (d), and (e), the difference oscillates, and at the same time the amplitude increases. Only for (a) and (c), the difference stays constant and is equal to 0. But these are the cases in which the position does not change at all, which is why we can deduct that the $x_0 = 0$ cancels the error that is responsible for the difference. As the theory predicts $x_0 \cos(\Omega t)$, we expect that the error lies in the frequency $\Omega$ such that our simulation does not fully compute the exact $\cos (\Omega t)$, but a small deviation $\cos (\Omega '(t) t)$ from that. This deviation of the true frequency is likely time-dependent. Similar reasoning is also expected for the variances. It has probably to do with the second-order product formula only being an approximation.\\

Overall one can summarize that the simulation method using the second-order product formula and discretizing the system yields results that are in good reasoning with the theory. However, it needs to be pointed out that any approximation is only an approximation which is why deviations will increase with more and more iterations. Additionally, numerical uncertainties are also a source of errors. Nevertheless, the method used is very viable to produce good results. The advantage of this simulation is how versatile it is, changing the initial condition or parameters can be done easily, whereas doing it analytically would take much more time.


